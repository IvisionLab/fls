import os
import sys
import json
import datetime
import re
import numpy as np
import tensorflow as tf
import keras
import keras.backend as K
import keras.layers as KL
import keras.engine as KE
import keras.models as KM
import rboxnet
from rboxnet import model, utils
from rboxnet.model import rpn_class_loss_graph, rpn_bbox_loss_graph
from rboxnet.model import log, fullmatch


def compute_rbox_deltas(box, rbox):
  box = tf.cast(box, tf.float32)
  rbox = tf.cast(rbox, tf.float32)

  height = box[:, 2] - box[:, 0]
  width = box[:, 3] - box[:, 1]

  dy1 = (rbox[:, 0] - box[:, 0]) / height
  dx1 = (rbox[:, 1] - box[:, 1]) / width
  dy2 = (box[:, 2] - rbox[:, 2]) / height
  dx2 = (box[:, 3] - rbox[:, 3]) / width

  result = tf.stack([dy1, dx1, dy2, dx2], axis=1)
  return result


def check_image_size(config):
  # Image size must be dividable by 2 multiple times
  h, w = config.IMAGE_SHAPE[:2]
  if h / 2**6 != int(h / 2**6) or w / 2**6 != int(w / 2**6):
    raise Exception("Image size must be dividable by 2 at least 6 times "
                    "to avoid fractions when downscaling and upscaling."
                    "For example, use 256, 320, 384, 448, 512, ... etc. ")


def upsample_layer(M, M_name, C, C_name):
  return [
      KL.UpSampling2D(size=(2, 2), name=M_name)(M),
      KL.Conv2D(256, (1, 1), name=C_name)(C)
  ]


def feature_extractor_layers(input_image, config):
  # Bottom-up Layers
  _, C2, C3, C4, C5 = rboxnet.model.resnet_graph(
      input_image, config.BACKBONE, stage5=True)

  # Top-down Layers
  M5 = KL.Conv2D(256, (1, 1), name='fpn_c5p5')(C5)
  M4 = KL.Add(name="fpn_p4add")(upsample_layer(M5, "fpn_p5upsampled", C4,
                                               "fpn_c4p4"))
  M3 = KL.Add(name="fpn_p3add")(upsample_layer(M4, "fpn_p4upsampled", C3,
                                               "fpn_c3p3"))
  M2 = KL.Add(name="fpn_p2add")(upsample_layer(M3, "fpn_p3upsampled", C2,
                                               "fpn_c2p2"))

  # Attach 3x3 conv to all P layers to get the final feature maps.
  P2 = KL.Conv2D(256, (3, 3), padding="SAME", name="fpn_p2")(M2)
  P3 = KL.Conv2D(256, (3, 3), padding="SAME", name="fpn_p3")(M3)
  P4 = KL.Conv2D(256, (3, 3), padding="SAME", name="fpn_p4")(M4)
  P5 = KL.Conv2D(256, (3, 3), padding="SAME", name="fpn_p5")(M5)
  # P6 is used for the 5th anchor scale in RPN.
  # Generated by subsampling from P5 with stride of 2.
  P6 = KL.MaxPooling2D(pool_size=(1, 1), strides=2, name="fpn_p6")(P5)

  return P2, P3, P4, P5, P6


def rpn_layers(rpn_feature_maps, anchors, config):
  # RPN Model
  rpn = model.build_rpn_model(config.RPN_ANCHOR_STRIDE,
                              len(config.RPN_ANCHOR_RATIOS), 256)

  # Loop through pyramid layers
  output_layers = []
  for p in rpn_feature_maps:
    output_layers.append(rpn([p]))

  # Concatenate layer outputs
  # Convert from list of lists of level outputs to list of lists of outputs across levels.
  # e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]]
  output_names = ["rpn_class_logits", "rpn_class", "rpn_bbox"]
  outputs = list(zip(*output_layers))
  rpn_class_logits, rpn_class, rpn_bbox = [
      KL.Concatenate(axis=1, name=n)(list(o))
      for o, n in zip(outputs, output_names)
  ]

  # Generate proposals
  # Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates and zero padded
  rpn_rois = model.ProposalLayer(
      proposal_count=config.POST_NMS_ROIS_TRAINING,
      nms_threshold=config.RPN_NMS_THRESHOLD,
      name="ROI",
      anchors=anchors,
      config=config)([rpn_class, rpn_bbox])

  return [rpn_class_logits, rpn_class, rpn_bbox, rpn_rois]


def rbox_class_loss_graph(target_class_ids, pred_class_logits,
                          active_class_ids):
  target_class_ids = tf.cast(target_class_ids, 'int64')

  # Find predictions of classes that are not in the dataset.
  pred_class_ids = tf.argmax(pred_class_logits, axis=2)
  pred_active = tf.gather(active_class_ids[0], pred_class_ids)

  # Loss
  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(
      labels=target_class_ids, logits=pred_class_logits)

  # Erase losses of predictions of classes that are not in the active
  # classes of the image.
  loss = loss * pred_active

  # Computer loss mean. Use only predictions that contribute
  # to the loss to get a correct mean.
  loss = tf.reduce_sum(loss) / tf.reduce_sum(pred_active)
  return loss


def rbox_bbox_loss_graph(target_bbox, target_class_ids, pred_bbox):
  # Reshape to merge batch and roi dimensions for simplicity.
  target_class_ids = K.reshape(target_class_ids, (-1, ))
  target_bbox = K.reshape(target_bbox, (-1, 4))
  pred_bbox = K.reshape(pred_bbox, (-1, K.int_shape(pred_bbox)[2], 4))

  # Only positive ROIs contribute to the loss. And only
  # the right class_id of each ROI. Get their indicies.
  positive_roi_ix = tf.where(target_class_ids > 0)[:, 0]
  positive_roi_class_ids = tf.cast(
      tf.gather(target_class_ids, positive_roi_ix), tf.int64)
  indices = tf.stack([positive_roi_ix, positive_roi_class_ids], axis=1)

  # Gather the deltas (predicted and true) that contribute to loss
  target_bbox = tf.gather(target_bbox, positive_roi_ix)
  pred_bbox = tf.gather_nd(pred_bbox, indices)

  # Smooth-L1 Loss
  loss = K.switch(
      tf.size(target_bbox) > 0,
      model.smooth_l1_loss(y_true=target_bbox, y_pred=pred_bbox),
      tf.constant(0.0))
  loss = K.mean(loss)
  loss = K.reshape(loss, [1, 1])
  return loss


def rpn_class_loss_layer(input_rpn_match, rpn_class_logits):
  return KL.Lambda(
      lambda x: rpn_class_loss_graph(*x),
      name="rpn_class_loss")([input_rpn_match, rpn_class_logits])


def rpn_bbox_loss_layer(config, input_rpn_bbox, input_rpn_match, rpn_bbox):
  return KL.Lambda(
      lambda x: rpn_bbox_loss_graph(config, *x),
      name="rpn_bbox_loss")([input_rpn_bbox, input_rpn_match, rpn_bbox])


def rbox_class_loss_layer(target_class_ids, rbox_class_logits,
                          active_class_ids):
  return KL.Lambda(
      lambda x: rbox_class_loss_graph(*x), name="rbox_class_loss")(
          [target_class_ids, rbox_class_logits, active_class_ids])


def rbox_bbox_loss_layer(target_bbox, target_class_ids, rbox_bbox):
  return KL.Lambda(
      lambda x: rbox_bbox_loss_graph(*x),
      name="rbox_bbox_loss")([target_bbox, target_class_ids, rbox_bbox])


def rbox_deltas_loss_layer(target_rbox_deltas, target_class_ids, rbox_deltas):
  return KL.Lambda(
      lambda x: model.custom_loss_graph(*x), name="rbox_deltas_loss")(
          [target_rbox_deltas, target_class_ids, rbox_deltas])

def rbox_angles_loss_layer(target_rbox_angles, target_class_ids, rbox_angles):
  return KL.Lambda(
      lambda x: model.custom_loss_graph(*x), name="rbox_angles_loss")(
          [target_rbox_angles, target_class_ids, rbox_angles])

def rbox_dim_loss_layer(target_rbox_dim, target_class_ids, rbox_dim):
  return KL.Lambda(
      lambda x: model.custom_loss_graph(*x), name="rbox_dim_loss")(
          [target_rbox_dim, target_class_ids, rbox_dim])

def roi_align_layers(rois, feature_maps, config):
  return model.PyramidROIAlign([config.POOL_SIZE, config.POOL_SIZE],
                               config.IMAGE_SHAPE,
                               name="roi_align")([rois] + feature_maps)


def full_connected_layers(x, config):
  # Two 1024 FC layers (implemented with Conv2D for consistency)
  x = KL.TimeDistributed(
      KL.Conv2D(1024, (config.POOL_SIZE, config.POOL_SIZE), padding="valid"),
      name="rbox_class_conv1")(x)

  x = KL.TimeDistributed(model.BatchNorm(axis=3), name='rbox_class_bn1')(x)
  x = KL.Activation('relu')(x)
  x = KL.TimeDistributed(KL.Conv2D(1024, (1, 1)), name="rbox_class_conv2")(x)
  x = KL.TimeDistributed(model.BatchNorm(axis=3), name='rbox_class_bn2')(x)
  x = KL.Activation('relu')(x)
  return KL.Lambda(
      lambda x: K.squeeze(K.squeeze(x, 3), 2), name="pool_squeeze")(x)


############################################################
#  Feature Pyramid Network Heads
############################################################
def bbox_classifier_layers(shared, num_classes):
  # Classifier head
  rbox_class_logits = KL.TimeDistributed(
      KL.Dense(num_classes), name='rbox_class_logits')(shared)
  rbox_probs = KL.TimeDistributed(
      KL.Activation("softmax"), name="rbox_class")(rbox_class_logits)

  # BBox head
  # [batch, boxes, num_classes * (dy, dx, log(dh), log(dw))]
  x = KL.TimeDistributed(
      KL.Dense(num_classes * 4, activation='linear'),
      name='rbox_bbox_fc')(shared)

  # Reshape to [batch, boxes, num_classes, (dy, dx, log(dh), log(dw))]
  s = K.int_shape(x)
  rbox_bbox = KL.Reshape((s[1], num_classes, 4), name="rbox_bbox")(x)

  return rbox_class_logits, rbox_probs, rbox_bbox


def rbox_deltas_regressor_layers(shared, num_classes):
  # rotated bounding box deltas
  x = KL.TimeDistributed(
      KL.Dense(num_classes * 4, activation='linear'),
      name='rbox_deltas_fc')(shared)

  s = K.int_shape(x)
  rbox_deltas = KL.Reshape((s[1], num_classes, 4), name="rbox_deltas")(x)
  return rbox_deltas

def rbox_rotdim_regressor_layers(shared, num_classes):
  # angles regressor
  x = KL.TimeDistributed(
      KL.Dense(num_classes * 1, activation='linear'),
      name='rbox_angles_fc')(shared)

  s = K.int_shape(x)
  rbox_angles = KL.Reshape((s[1], num_classes, 1), name="rbox_angles")(x)

  # dimension (width and height) regressor
  x = KL.TimeDistributed(
      KL.Dense(num_classes * 2, activation='linear'),
      name='rbox_dim_fc')(shared)

  s = K.int_shape(x)
  rbox_dim = KL.Reshape((s[1], num_classes, 2), name="rbox_dim")(x)

  return rbox_angles, rbox_dim

def detection_target_layer(config, inputs):
  return DetectionTargetLayer(config, name="proposal_targets")(inputs)


def detection_targets_graph(config, proposals, gt_class_ids, gt_boxes,
                            **kwargs):
  # Assertions
  asserts = \
      [tf.Assert(tf.greater(tf.shape(proposals)[0], 0), [proposals], name="roi_assertion"),]

  with tf.control_dependencies(asserts):
    proposals = tf.identity(proposals)

  # Remove zero padding
  proposals, _ = \
      model.trim_zeros_graph(proposals, name="trim_proposals")
  gt_boxes, non_zeros = \
      model.trim_zeros_graph(gt_boxes, name="trim_gt_boxes")
  gt_class_ids = \
      model.tf.boolean_mask(gt_class_ids, non_zeros, name="trim_gt_class_ids")

  # Handle COCO crowds
  # A crowd box in COCO is a bounding box around several instances. Exclude
  # them from training. A crowd box is given a negative class ID.
  crowd_ix = tf.where(gt_class_ids < 0)[:, 0]
  non_crowd_ix = tf.where(gt_class_ids > 0)[:, 0]
  crowd_boxes = tf.gather(gt_boxes, crowd_ix)
  gt_class_ids = tf.gather(gt_class_ids, non_crowd_ix)
  gt_boxes = tf.gather(gt_boxes, non_crowd_ix)

  # Compute overlaps matrix [proposals, gt_boxes]
  overlaps = model.overlaps_graph(proposals, gt_boxes)

  # Compute overlaps with crowd boxes [anchors, crowds]
  crowd_overlaps = model.overlaps_graph(proposals, crowd_boxes)
  crowd_iou_max = tf.reduce_max(crowd_overlaps, axis=1)
  no_crowd_bool = (crowd_iou_max < 0.001)

  # Determine postive and negative ROIs
  roi_iou_max = tf.reduce_max(overlaps, axis=1)
  # 1. Positive ROIs are those with >= 0.5 IoU with a GT box
  positive_roi_bool = (roi_iou_max >= 0.5)
  positive_indices = tf.where(positive_roi_bool)[:, 0]
  # 2. Negative ROIs are those with < 0.5 with every GT box. Skip crowds.
  negative_indices = tf.where(
      tf.logical_and(roi_iou_max < 0.5, no_crowd_bool))[:, 0]

  # Subsample ROIs. Aim for 33% positive
  # Positive ROIs
  positive_count = int(config.TRAIN_ROIS_PER_IMAGE * config.ROI_POSITIVE_RATIO)
  positive_indices = tf.random_shuffle(positive_indices)[:positive_count]
  positive_count = tf.shape(positive_indices)[0]
  # Negative ROIs. Add enough to maintain positive:negative ratio.
  r = 1.0 / config.ROI_POSITIVE_RATIO
  negative_count = tf.cast(r * tf.cast(positive_count, tf.float32),
                           tf.int32) - positive_count
  negative_indices = tf.random_shuffle(negative_indices)[:negative_count]
  # Gather selected ROIs
  positive_rois = tf.gather(proposals, positive_indices)
  negative_rois = tf.gather(proposals, negative_indices)

  # Assign positive ROIs to GT boxes.
  positive_overlaps = tf.gather(overlaps, positive_indices)
  roi_gt_box_assignment = tf.argmax(positive_overlaps, axis=1)
  roi_gt_boxes = tf.gather(gt_boxes, roi_gt_box_assignment)
  roi_gt_class_ids = tf.gather(gt_class_ids, roi_gt_box_assignment)

  # Compute bbox refinement for positive ROIs
  deltas = utils.box_refinement_graph(positive_rois, roi_gt_boxes)
  deltas /= config.BBOX_STD_DEV

  # Compute mask targets
  boxes = positive_rois

  # Append negative ROIs and pad bbox deltas and masks that
  # are not used for negative ROIs with zeros.
  rois = tf.concat([positive_rois, negative_rois], axis=0)
  N = tf.shape(negative_rois)[0]
  P = tf.maximum(config.TRAIN_ROIS_PER_IMAGE - tf.shape(rois)[0], 0)
  rois = tf.pad(rois, [(0, P), (0, 0)])
  roi_gt_class_ids = tf.pad(roi_gt_class_ids, [(0, N + P)])
  deltas = tf.pad(deltas, [(0, N + P), (0, 0)])

  if 'gt_rboxes' in kwargs:
    gt_rboxes = kwargs.get('gt_rboxes')
    gt_rboxes, _ = model.trim_zeros_graph(gt_rboxes, name="trim_gt_rboxes")
    gt_rboxes = tf.gather(gt_rboxes, non_crowd_ix)
    roi_gt_rboxes = tf.gather(gt_rboxes, roi_gt_box_assignment)

    if config.regressor == "deltas":
      rbox_deltas = compute_rbox_deltas(roi_gt_boxes, roi_gt_rboxes)
      rbox_deltas /= 0.2
      rbox_deltas = tf.pad(rbox_deltas, [(0, N + P), (0, 0)])
      return rois, roi_gt_class_ids, deltas, rbox_deltas
    elif config.regressor == "rotdim":
      gt_angles = kwargs.get('gt_angles')
      gt_angles = tf.boolean_mask(gt_angles, non_zeros, name="trim_gt_angles")
      gt_angles = tf.gather(gt_angles, non_crowd_ix)
      roi_gt_angles = tf.gather(gt_angles, roi_gt_box_assignment)
      rbox_angles = roi_gt_angles / 0.2

      # point 1
      y1 = roi_gt_rboxes[:, 0]
      x1 = roi_gt_rboxes[:, 1]
      # point 2
      y2 = roi_gt_rboxes[:, 2]
      x2 = roi_gt_rboxes[:, 3]
      # point 3
      y3 = roi_gt_rboxes[:, 4]
      x3 = roi_gt_rboxes[:, 5]

      rw = tf.sqrt(tf.pow(x2 - x1, 2) + tf.pow(y2 - y1, 2))
      rh = tf.sqrt(tf.pow(x3 - x2, 2) + tf.pow(y3 - y2, 2))
      rbox_dim = tf.stack([rh, rw], axis=1)
      rbox_dim /= 0.1

      rbox_dim = tf.pad(rbox_dim, [(0, N + P), (0, 0)])
      rbox_angles = tf.pad(rbox_angles, [(0, N + P), (0, 0)])
      return rois, roi_gt_class_ids, deltas, rbox_angles, rbox_dim

  return rois, roi_gt_class_ids, deltas


############################################################
#  Base Class
############################################################
class DetectionTargetLayer(KE.Layer):
  def __init__(self, config, **kwargs):
    super(DetectionTargetLayer, self).__init__(**kwargs)
    self.config = config

  def call(self, inputs):
    names = ["rois", "target_class_ids", "target_bbox"]
    if self.config.regressor == "deltas":
      names += ["target_rbox_deltas"]
      outputs = utils.batch_slice(
          inputs,
          lambda a, b, c, d: detection_targets_graph(self.config, a, b, c, gt_rboxes=d),
          self.config.IMAGES_PER_GPU,
          names=names)
    elif self.config.regressor == "rotdim":
      names += ["target_rbox_angles", "target_rbox_dim"]
      outputs = utils.batch_slice(
          inputs,
          lambda a, b, c, d, e: detection_targets_graph(self.config, a, b, c, gt_rboxes=d, gt_angles=e),
          self.config.IMAGES_PER_GPU,
          names=names)
    else:
      outputs = utils.batch_slice(
          inputs,
          lambda a, b, c: detection_targets_graph(self.config, a, b, c),
          self.config.IMAGES_PER_GPU,
          names=names)

    return outputs

  def compute_output_shape(self, input_shape):

    output_shape = [
        (None, self.config.TRAIN_ROIS_PER_IMAGE, 4),  # rois
        (None, 1),  # class_ids
        (None, self.config.TRAIN_ROIS_PER_IMAGE, 4),  # deltas
    ]

    if self.config.regressor == "deltas":
      output_shape += [(None, self.config.TRAIN_ROIS_PER_IMAGE, 4)]  # RBOX deltas
    elif self.config.regressor == "rotdim":
      output_shape += [
          (None, self.config.TRAIN_ROIS_PER_IMAGE, 1),  # RBOX angle
          (None, self.config.TRAIN_ROIS_PER_IMAGE, 2)
      ]  # RBOX dimensions

    return output_shape

  def compute_mask(self, inputs, mask=None):
    mask = [None, None, None]

    if self.config.regressor == "deltas":
      mask += [None]
    elif self.config.regressor == "rotdim":
      mask += [None, None]

    return mask


############################################################
#  Base Class
############################################################
class Base(object):
  def __init__(self, keras_model, config):
    self.config = config
    self.model_dir = os.path.join(os.getcwd(), "logs")
    self.set_log_dir()
    self.keras_model = keras_model

  def set_log_dir(self, model_path=None):
    # Set date and epoch counter as if starting a new model
    self.epoch = 0
    now = datetime.datetime.now()
    # If we have a model path with date and epochs use them
    if model_path:
      regex = r".*/\w+(\d{4})(\d{2})(\d{2})T(\d{2})(\d{2})/rboxnet\_\w+(\d{4})\.h5"
      m = re.match(regex, model_path)
      if m:
        now = datetime.datetime(
            int(m.group(1)), int(m.group(2)), int(m.group(3)), int(m.group(4)),
            int(m.group(5)))
        self.epoch = int(m.group(6)) + 1

    # Directory for training logs
    self.log_dir = os.path.join(
        self.model_dir, "{}{:%Y%m%dT%H%M}".format(self.config.NAME.lower(),
                                                  now))

    # Path to save after each epoch. Include placeholders that get filled by Keras.
    self.checkpoint_path = os.path.join(
        self.log_dir, "rboxnet_{}_*epoch*.h5".format(self.config.NAME.lower()))
    self.checkpoint_path = self.checkpoint_path.replace(
        "*epoch*", "{epoch:04d}")

  def set_trainable(self, layer_regex, keras_model=None, indent=0, verbose=1):
    # Print message on the first call (but not on recursive calls)
    if verbose > 0 and keras_model is None:
      log("Selecting layers to train")

    keras_model = keras_model or self.keras_model

    # In multi-GPU training, we wrap the model. Get layers
    # of the inner model because they have the weights.
    layers = keras_model.inner_model.layers if hasattr(
        keras_model, "inner_model") else keras_model.layers

    for layer in layers:
      # Is the layer a model?
      if layer.__class__.__name__ == 'Model':
        print("In model: ", layer.name)
        self.set_trainable(layer_regex, keras_model=layer, indent=indent + 4)
        continue

      if not layer.weights:
        continue

      # Is it trainable?
      trainable = bool(fullmatch(layer_regex, layer.name))
      # Update layer. If layer is a container, update inner layer.
      if layer.__class__.__name__ == 'TimeDistributed':
        layer.layer.trainable = trainable
      else:
        layer.trainable = trainable
      # Print trainble layer names
      if trainable and verbose > 0:
        log("{}{:20}   ({})".format(" " * indent, layer.name,
                                    layer.__class__.__name__))

  def load_weights(self, filepath, by_name=False, exclude=None):
    import h5py
    from keras.engine import saving

    if exclude:
      by_name = True

    if h5py is None:
      raise ImportError('`load_weights` requires h5py.')

    f = h5py.File(filepath, mode='r')
    if 'layer_names' not in f.attrs and 'model_weights' in f:
      f = f['model_weights']

    # In multi-GPU training, we wrap the model. Get layers
    # of the inner model because they have the weights.
    keras_model = self.keras_model
    layers = keras_model.inner_model.layers if hasattr(keras_model, "inner_model")\
        else keras_model.layers

    # Exclude some layers
    if exclude:
      layers = filter(lambda l: l.name not in exclude, layers)

    if by_name:
      saving.load_weights_from_hdf5_group_by_name(f, layers)
    else:
      saving.load_weights_from_hdf5_group(f, layers)
    if hasattr(f, 'close'):
      f.close()

    # Update the log directory
    self.set_log_dir(filepath)

  def find_last(self):
    # Get directory names. Each directory corresponds to a model
    dir_names = next(os.walk(self.model_dir))[1]
    key = self.config.NAME.lower()
    dir_names = filter(lambda f: f.startswith(key), dir_names)
    dir_names = sorted(dir_names)
    if not dir_names:
      return None, None
    # Pick last directory
    dir_name = os.path.join(self.model_dir, dir_names[-1])
    # Find the last checkpoint
    checkpoints = next(os.walk(dir_name))[2]
    checkpoints = filter(lambda f: f.startswith("rboxnet"), checkpoints)
    checkpoints = sorted(checkpoints)
    if not checkpoints:
      return dir_name, None
    checkpoint = os.path.join(dir_name, checkpoints[-1])
    return dir_name, checkpoint

  def mold_inputs(self, images):
    molded_images = []
    image_metas = []
    windows = []
    for image in images:
      # Resize image to fit the model expected size
      # TODO: move resizing to mold_image()
      molded_image, window, scale, padding = \
          utils.resize_image(
              image,
              min_dim=self.config.IMAGE_MIN_DIM,
              max_dim=self.config.IMAGE_MAX_DIM,
              padding=self.config.IMAGE_PADDING)

      molded_image = model.mold_image(molded_image, self.config)

      # Build image_meta
      image_meta = model.compose_image_meta(
          0, image.shape, window,
          np.zeros([self.config.NUM_CLASSES], dtype=np.int32))

      # Append
      molded_images.append(molded_image)
      windows.append(window)
      image_metas.append(image_meta)

    # Pack into arrays
    molded_images = np.stack(molded_images)
    image_metas = np.stack(image_metas)
    windows = np.stack(windows)
    return molded_images, image_metas, windows
